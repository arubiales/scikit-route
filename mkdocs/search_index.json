{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to Scikit Route\n\n\nScikit Route\n is a Python module to optimize route and travel problems and there are a lot of different alogithms in the package for all the different purposes.\n\n\nScikit Route have the spirit and the soul of Scikit Learn\n wich mean that is fully oriented to the programmer, easy to use and to understund, you can create algorithm with a few lines of code, and the modules, submodules, methods of the algorithms, etc are the same than Scikit Learn wich will make even easyer to use this library if you previously know Scikit Learn.\n\n\nFor a quick introduction to using Skleran Route, please refer to the \nTutorial\n. For a more advanced introduction which describes please refer to the each one of the modules in the \nAPI Documentation\n.\n\n\nGetting started\n\n\n\n\nInstallation instructions\n\n\nTutorial\n\n\n\n\nAPI documentation\n\n\n\n\nDatasets\n\n\nPreprocessing\n\n\nClustering\n\n\nGenetics\n\n\nSimmulated Annealing\n\n\nBrute Force\n\n\nUtilities",
            "title": "Home"
        },
        {
            "location": "/#welcome-to-scikit-route",
            "text": "Scikit Route  is a Python module to optimize route and travel problems and there are a lot of different alogithms in the package for all the different purposes.  Scikit Route have the spirit and the soul of Scikit Learn  wich mean that is fully oriented to the programmer, easy to use and to understund, you can create algorithm with a few lines of code, and the modules, submodules, methods of the algorithms, etc are the same than Scikit Learn wich will make even easyer to use this library if you previously know Scikit Learn.  For a quick introduction to using Skleran Route, please refer to the  Tutorial . For a more advanced introduction which describes please refer to the each one of the modules in the  API Documentation .",
            "title": "Welcome to Scikit Route"
        },
        {
            "location": "/#getting-started",
            "text": "Installation instructions  Tutorial",
            "title": "Getting started"
        },
        {
            "location": "/#api-documentation",
            "text": "Datasets  Preprocessing  Clustering  Genetics  Simmulated Annealing  Brute Force  Utilities",
            "title": "API documentation"
        },
        {
            "location": "/BruteForce/",
            "text": "Brute Force\n\n\nThe main advantage of this Brute Force algorithm over others solutions are:\n\n\n\n\nEasy to learn and to use, inspired in Scikit Learn library.\n\n\nHeuristic aproach, wicht mean it's deterministic, and check all possible solutions to give the best.\n\n\nVery well documented.\n\n\n\n\nThis algorithm is very easy to understund, it takes all the possible combinations and choose the best one.\n\n\nBrute Force\n\n\nHyper Parameters\n\n\n\n\nsklearn_route.brute.BruteForce\n(max_time_work=8.0, extra_cost=10.0, people=1)\n\n\n\n\n\n\n\n\nmax_time_work\n: float32, default=8\n\n\n\n\nThe number of ours that a employ can work per day. For example\nif it's 8 hours, the algorithm will force that a route have to\nfinish after the 8 hours have been completed, making the\nemployeed come back home. it's a time constraint.\n\n\n\n\n\n\n\n\nextra_cost\n: float32, default=0\n\n\n\n\nIf it's 0 anything happend. If it's > 0 in combination with\nmax_time_work when the max_time_work is reached, extra_cost is\napplied. This add a extra cost to the solution each time that\nmax_time_work is reached. It's like extra pay for the worker \neach time max_time_work is completed (journey).\n\n\n\n\n\n\n\n\npeople\n int32, default=1\n\n\n\n\nThe number of people that you use in each route, for example if\nyou need two truck drivers. that's another contstraint. That\nwill multiply the time_costs and the extra_cost. Not the travel\ncost because it's assumed that both go in the same vehicle.\n\n\n\n\n\n\n\n\nMethod\n\n\n\n\n.fit(route_example, time_costs, fuel_costs)\n:\n\n\nThis method train the algorithm, we need to pass the following data:\n\n        1. \nroute_example\n: Is a list random route to initiate the algorithm.\n        2. \ntime_cost\n: it's a dict of dicts that represent a diagonal matrix with the times between all points\n        3. \nfuel_cost\n: it's a dict of dicts that represent a diagonal matrix with the costs between all points\n\n\n\n\n\n\n\n\nExample with Brute Force algorithm.\n\n\nfrom sklearn_route.datasets import load_alicante_murcia\nfrom sklearn_route.preprocessing import matrix_to_dict\nfrom sklearn_route.brute import BruteForce\n\ndf_alicante_murcia = load_alicante_murcia()\n\n#Dataset - id of origin - id of destiny - column to transform (in this case the hour)\ntime_matrix = matrix_to_dict(df_alicante_murcia, \"id_origin\", \"id_destinity\", \"hora\")\n\n#Dataset - id of origin - id of destiny - column to transform (in this case the cost)\ncost_matrix = matrix_to_dict(df_barcedf_alicante_murcialona, \"id_origin\", \"id_destinity\", \"hora\")\n\n#Create a random route, it's will needed to initiate the algorithm\nroute_example = list(dict.fromkeys(cost_matrix_df).keys())\n\n#Instantiate the algorithm\nbf = BruteForce(max_time_work=6, extra_cost=12.83)\n\n#random route - time_matrix - cost_matrix\nresult = bf.fit(route_example, time_matrix, cost_matrix)\n\n#Printing the best route\nprint(result)\n\n\n\nUnderstunding the data needed by the algorithm\n\n\nThe \ndicts of dicts\n time_cost and fuel_cost, are dicts of dicts for performance reason. these dicst could be also represented as diagonal symmetric matrix, columns and index represent the cost of the point. Here an example:\n\n\neasy_route = {\n            1:{\n                1:0,\n                2:x\n                3:y\n                },\n            2:{\n                1:x,\n                2:0,\n                3:z\n                },\n            3:{\n                1:y,\n                2:z,\n                3:0\n                }\n            }\n\n#We can see clearly the matrix with pandas\nimport pandas as pd\n\n#Visualice the diagonal symmetric matrix\nprint(pd.DataFrame(easy_route))\n\n\n\nOutput:\n\n\n\n\nIt's important to understund that the cost matrix \nmust be the computation of all the costs\n that the user of the algorithm want to take in count \nper person\n. In the majority of cases the cost will be composed by:\n\n\n\n\nThe cost of the fuel for one person to go from one point to another.\n\n\nThe cost of the hours of work needed from one point to another.\n\n\n\n\nIf we have multiple persons doing the same route, the \npeople\n parameter will multiply the cost by the number of persons and will increase the cost with the \nextra_cost\n in case of the \nmax_time_work\n paremeter will be surpased.\n\n\nAlso note that \nextra_cost\n \ncould be used as a maximun capacity\n of a truck for example in the case of material transports problems.\n\n\nThe other dict of dicts \ntime_matrix\n parameter will be only used by the algorithm to compute the \nmax_time_work\n and compute the final route of the algorithm with the times that the salesman/truck have finished their journey.\n\n\nCaveats for beginners\n\n\nIt's always common for beginners think \"If I use this algorithm I will get the best result (not a suboptimal like with others metaheuristics approachs)\" This is true but the main problem of this algorithm  is that is very computationally expensive, for routes with more than 15 places you will need to make \nmore than 1 Trillion operations\n (15 factorial because the first one don't count, because is always home) that's imposible to do for machines today, even for 15 (14!) places will be 87 Billions operations, probably it's needed 1 day of computation. That's why we encourage to not use on routes with \nmore than twelve places\n and never use it with more than 15.",
            "title": "BruteForce"
        },
        {
            "location": "/BruteForce/#brute-force",
            "text": "The main advantage of this Brute Force algorithm over others solutions are:   Easy to learn and to use, inspired in Scikit Learn library.  Heuristic aproach, wicht mean it's deterministic, and check all possible solutions to give the best.  Very well documented.   This algorithm is very easy to understund, it takes all the possible combinations and choose the best one.",
            "title": "Brute Force"
        },
        {
            "location": "/BruteForce/#brute-force_1",
            "text": "",
            "title": "Brute Force"
        },
        {
            "location": "/BruteForce/#hyper-parameters",
            "text": "sklearn_route.brute.BruteForce (max_time_work=8.0, extra_cost=10.0, people=1)     max_time_work : float32, default=8   The number of ours that a employ can work per day. For example\nif it's 8 hours, the algorithm will force that a route have to\nfinish after the 8 hours have been completed, making the\nemployeed come back home. it's a time constraint.     extra_cost : float32, default=0   If it's 0 anything happend. If it's > 0 in combination with\nmax_time_work when the max_time_work is reached, extra_cost is\napplied. This add a extra cost to the solution each time that\nmax_time_work is reached. It's like extra pay for the worker \neach time max_time_work is completed (journey).     people  int32, default=1   The number of people that you use in each route, for example if\nyou need two truck drivers. that's another contstraint. That\nwill multiply the time_costs and the extra_cost. Not the travel\ncost because it's assumed that both go in the same vehicle.",
            "title": "Hyper Parameters"
        },
        {
            "location": "/BruteForce/#method",
            "text": ".fit(route_example, time_costs, fuel_costs) :  This method train the algorithm, we need to pass the following data: \n        1.  route_example : Is a list random route to initiate the algorithm.\n        2.  time_cost : it's a dict of dicts that represent a diagonal matrix with the times between all points\n        3.  fuel_cost : it's a dict of dicts that represent a diagonal matrix with the costs between all points",
            "title": "Method"
        },
        {
            "location": "/BruteForce/#example-with-brute-force-algorithm",
            "text": "from sklearn_route.datasets import load_alicante_murcia\nfrom sklearn_route.preprocessing import matrix_to_dict\nfrom sklearn_route.brute import BruteForce\n\ndf_alicante_murcia = load_alicante_murcia()\n\n#Dataset - id of origin - id of destiny - column to transform (in this case the hour)\ntime_matrix = matrix_to_dict(df_alicante_murcia, \"id_origin\", \"id_destinity\", \"hora\")\n\n#Dataset - id of origin - id of destiny - column to transform (in this case the cost)\ncost_matrix = matrix_to_dict(df_barcedf_alicante_murcialona, \"id_origin\", \"id_destinity\", \"hora\")\n\n#Create a random route, it's will needed to initiate the algorithm\nroute_example = list(dict.fromkeys(cost_matrix_df).keys())\n\n#Instantiate the algorithm\nbf = BruteForce(max_time_work=6, extra_cost=12.83)\n\n#random route - time_matrix - cost_matrix\nresult = bf.fit(route_example, time_matrix, cost_matrix)\n\n#Printing the best route\nprint(result)",
            "title": "Example with Brute Force algorithm."
        },
        {
            "location": "/BruteForce/#understunding-the-data-needed-by-the-algorithm",
            "text": "The  dicts of dicts  time_cost and fuel_cost, are dicts of dicts for performance reason. these dicst could be also represented as diagonal symmetric matrix, columns and index represent the cost of the point. Here an example:  easy_route = {\n            1:{\n                1:0,\n                2:x\n                3:y\n                },\n            2:{\n                1:x,\n                2:0,\n                3:z\n                },\n            3:{\n                1:y,\n                2:z,\n                3:0\n                }\n            }\n\n#We can see clearly the matrix with pandas\nimport pandas as pd\n\n#Visualice the diagonal symmetric matrix\nprint(pd.DataFrame(easy_route))  Output:   It's important to understund that the cost matrix  must be the computation of all the costs  that the user of the algorithm want to take in count  per person . In the majority of cases the cost will be composed by:   The cost of the fuel for one person to go from one point to another.  The cost of the hours of work needed from one point to another.   If we have multiple persons doing the same route, the  people  parameter will multiply the cost by the number of persons and will increase the cost with the  extra_cost  in case of the  max_time_work  paremeter will be surpased.  Also note that  extra_cost   could be used as a maximun capacity  of a truck for example in the case of material transports problems.  The other dict of dicts  time_matrix  parameter will be only used by the algorithm to compute the  max_time_work  and compute the final route of the algorithm with the times that the salesman/truck have finished their journey.",
            "title": "Understunding the data needed by the algorithm"
        },
        {
            "location": "/BruteForce/#caveats-for-beginners",
            "text": "It's always common for beginners think \"If I use this algorithm I will get the best result (not a suboptimal like with others metaheuristics approachs)\" This is true but the main problem of this algorithm  is that is very computationally expensive, for routes with more than 15 places you will need to make  more than 1 Trillion operations  (15 factorial because the first one don't count, because is always home) that's imposible to do for machines today, even for 15 (14!) places will be 87 Billions operations, probably it's needed 1 day of computation. That's why we encourage to not use on routes with  more than twelve places  and never use it with more than 15.",
            "title": "Caveats for beginners"
        },
        {
            "location": "/Clustering/",
            "text": "",
            "title": "Clustering"
        },
        {
            "location": "/Datasets/",
            "text": "Dataset\n\n\nThe dataset module are some datasets builded and adapted to the package algorithms. The purpose is to have a fast and easy understunding of the package, how the algorithms works, what kind of data is required and in wich format. \n\n\nThe data are different Datasets wich have the main info needed for the algorithm (not all the information is needed for every algorithm). The columns are:\n\n\n\n\nid_origin\n and \nid_destinity\n: point id that connect other point id\n\n\nlat_origin\n, \nlon_origin\n, \nlat_destinity\n, \nlon_destinity\n: The same as above, but with latitude and longitude\n\n\norigin\n and \ndestinity\n: The same as above but with the address\n\n\nmeters\n: distance in meters from the \nid_origin\n to the \nid_destinity\n\n\nsecs\n: distance in sec from the \nid_origin\n to the \nid_destinity\n\n\nhours\n: distance in hours from the \nid_origin\n to the \nid_destinity\n\n\nkilometers\n: distance in kilometers from the \nid_origin\n to the \nid_destinity\n\n\ncost\n: fuel cost, this is computation of the hours multiply for the cost per hour, and the fuel cost needed for make the route.\n\n\n\n\n\n\nThere are differents datasets you can import and load easy, one time it's loaded the data can be adapted with the preprocessing module if it's required. The datasets are \nPandas DataFrame\n objects\n\n\nfrom sklear_route.dataset import load_alicante_murcia\nfrom sklear_route.dataset import load_barcelona\nfrom sklear_route.dataset import load_madrid\nfrom sklear_route.dataset import load_valencia\nfrom sklearn.preprocessing import matrix_to_dict\n\n#Loading distinct datasets\ndf_alicante_murcia  = load_alicante_murcia()\ndf_barcelona = load_barcelona()\ndf_madrid = load_madrid()\ndf_valencia = load_valencia()\n\n#Converting a dataset to a diagonal symmetric matrix dict of dicts\ncost_matrix = matrix_to_dict(df_barcelona, \"id_origin\", \"id_destinity\", \"hora\")",
            "title": "Datasets"
        },
        {
            "location": "/Datasets/#dataset",
            "text": "The dataset module are some datasets builded and adapted to the package algorithms. The purpose is to have a fast and easy understunding of the package, how the algorithms works, what kind of data is required and in wich format.   The data are different Datasets wich have the main info needed for the algorithm (not all the information is needed for every algorithm). The columns are:   id_origin  and  id_destinity : point id that connect other point id  lat_origin ,  lon_origin ,  lat_destinity ,  lon_destinity : The same as above, but with latitude and longitude  origin  and  destinity : The same as above but with the address  meters : distance in meters from the  id_origin  to the  id_destinity  secs : distance in sec from the  id_origin  to the  id_destinity  hours : distance in hours from the  id_origin  to the  id_destinity  kilometers : distance in kilometers from the  id_origin  to the  id_destinity  cost : fuel cost, this is computation of the hours multiply for the cost per hour, and the fuel cost needed for make the route.    There are differents datasets you can import and load easy, one time it's loaded the data can be adapted with the preprocessing module if it's required. The datasets are  Pandas DataFrame  objects  from sklear_route.dataset import load_alicante_murcia\nfrom sklear_route.dataset import load_barcelona\nfrom sklear_route.dataset import load_madrid\nfrom sklear_route.dataset import load_valencia\nfrom sklearn.preprocessing import matrix_to_dict\n\n#Loading distinct datasets\ndf_alicante_murcia  = load_alicante_murcia()\ndf_barcelona = load_barcelona()\ndf_madrid = load_madrid()\ndf_valencia = load_valencia()\n\n#Converting a dataset to a diagonal symmetric matrix dict of dicts\ncost_matrix = matrix_to_dict(df_barcelona, \"id_origin\", \"id_destinity\", \"hora\")",
            "title": "Dataset"
        },
        {
            "location": "/Genetics/",
            "text": "Genetics\n\n\nThe main advantage of this Genetic algorithm over others solutions are:\n* Easy to learn and to use, inspired in Scikit Learn library.\n* Amazingly fast, builted in Cpython results with 400 populations and more than 1000 generations are done in less than a half of a second..\n* Very well documented.\n\n\nGenetic Algorithm is a metaheuristic aproach wich mean is not deterministic, this lead that every time you run the result could be different but will be very near of the \nglobal optimum\n.\n\n\nThis algorithm is called Genetic because is based in the Darwyn Theory of Evolution and the natural selection. This theory mainly says that \nin a population of individuals, the best ones have more probabilities to survive than others.\n. This in our problems mean that we have one \ninitial random route\n (individual), from this initialization we create a  list of \nrandom routes\n (random population), we are now in the \ngeneration 0\n. so the routes (individuals) of this population, as happend in reality, have the probability of mix one routes wich others and create new routes (have children), and this new routes will have mutations and crossover for each pair of routes (parents) (they have genetics modifications). So one time that all routes of the initial list of routes (random population) have pass through this phase (or not it's depend if they have childrens or not), we will finish the generation. when we finish the generation the worst route/s (individuals) will be replaced by the best route/s (this is the natural selection, the worst individuals die.) and will start the next generation with the new routes. And this will be repeated a number \nX\n of times (generations).\n\n\nGenetic\n\n\nHyper - Parameters\n\n\n\n\nsklearn_route.metaheuristics.genetics.Genetic\n(p_c=0.6, p_m=0.4, pop=400, gen=1600, k=3, early_stopping=None,\n                max_time_work=8.0, extra_cost=10.0, people=1)\n\n\n\n\n\n\n\n\np_c\n: float32, default=0.7\n\n\n\n\nBetween 0 and 1 the probability of crossover. In each generation\nat start the algorithm generate two random probabilities these random probabilies are the choosen when crossover will take place. If the random number is 4 that mean that the number in position and the following numbers will be now at start. And the first four numbers will be at the end.\n\nIf this ratio is very low\n (for example 0.1) will mean that a few new routes will be introduced (no childs in the population), so the optimization \nwill take more time, but the algorithm will be more stable\n. Instead \nif it's too hight\n (for example 0.9) will mean that the majority of the population will be changed per generation (new childs) this \nwill lead to a fast optimization but\n will be more inestable so the algorithm \ncould not converge\n.\n\n\n\n\n\n\n\n\np_m\n: float32, default=0.4\n\n\n\n\nIt's mutate, random probabilies are choosen when it will take\nplace. If the random numbers are 2 and 8 for example, the \nnumbers located at that index will swap positions.\n\nThe same as croosover (\np_c\n) a \nhigher numbers\n (0.9) \nwill lead to\n a lot of variation and a \nfaster optimization\n in the routes (children) \nbut\n also can make that the algorithm \ncould never converge.\nInstead \nlower numbers\n will lead to a \nmore stable algorithm but\n the alogorithm \nwill be very slow\n to converge.\n\n\n\n\n\n\n\n\npop\n: int32, default=400  \n\n\n\n\nThe number of population (different random solutions), more\npopulation, more probabilities to find different solutions but \nmake the algorithm more time expensive. If the number is too\nlow, the solution will be bad, because the algorithm can't \niterate over the loss function with the enough amount of data\n\n\n\n\n\n\n\n\ngen\n: int32 Default=1000\n\n\n\n\nThe number of generations. More generations make the algorithm\nachieve better solution but also make the algorithm take more\ntime. This can be solved with early_stoping parameter to make\nthe algorithm stop when is not improving. If the number of\ngeneration is low the algorithm may not converge.\n\n\n\n\n\n\n\n\nk\n: int32, default=3\n\n\n\n\nThe number of individual populations (solutions) that\nwill fight for being the best. When we are inside of a\ngeneration the algorithm take a subsample of numbers k. The best\nk (individual) solution is choosen as a parten for this generation.\n\n\n\n\n\n\n\n\nearly_stopping\n: int32, default=None\n\n\n\n\nIf it's None, the algorithm will finish when the last generation \nis performed. If it's other unsigned integer (positive integer\nnumber) if the algorithm can't improve the result while X\ngenerations (where x is early_stopping number) the algorithm\nwill stop.\n\n\n\n\n\n\n\n\nmax_time_work\n: float32, default=8\n\n\n\n\nThe number of ours that a employ can work per day. For example\nif it's 8 hours, the algorithm will force that a route have to\nfinish after the 8 hours have been completed, making the\nemployeed come back home. it's a time constraint.\n\n\n\n\n\n\n\n\nextra_cost\n: float32, default=0\n\n\n\n\nIf it's 0 anything happend. If it's > 0 in combination with\nmax_time_work when the max_time_work is reached, extra_cost is\napplied. This add a extra cost to the solution each time that\nmax_time_work is reached. It's like extra pay for the worker \neach time max_time_work is completed (journey).\n\n\n\n\n\n\n\n\npeople\n int32, default=1\n\n\n\n\nThe number of people that you use in each route, for example if\nyou need two truck drivers. that's another contstraint. That\nwill multiply the time_costs and the extra_cost. Not the travel\ncost because it's assumed that both go in the same vehicle.\n\n\n\n\n\n\n\n\nMethod\n\n\n\n\n.fit(route_example, time_costs, fuel_costs)\n:\n\n\nThis method train the algorithm, we need to pass the following data:\n\n        1. \nroute_example\n: Is a list random route to initiate the algorithm.\n        2. \ntime_cost\n: it's a dict of dicts that represent a diagonal matrix with the times between all points\n        3. \nfuel_cost\n: it's a dict of dicts that represent a diagonal matrix with the costs between all points\n\n\n\n\n\n\n\n\nAttribute\n\n\n\n\nhistory_\n:\n\n\nit's a list with the best cost in each generation. The loss function.\n\n\n\n\n\n\n\n\nExample with Genetic Algorithm.\n\n\nfrom sklearn_route.datasets import load_barcelona\nfrom sklearn_route.preprocessing import matrix_to_dict\nfrom sklearn_route.metaheuristics.genetics import Genetic\n\ndf_barcelona = load_barcelona()\n\n#Dataset - id of origin - id of destiny - column to transform (in this case the hour)\ntime_matrix = matrix_to_dict(df_barcelona, \"id_origin\", \"id_destinity\", \"hora\")\n\n#Dataset - id of origin - id of destiny - column to transform (in this case the cost)\ncost_matrix = matrix_to_dict(df_barcelona, \"id_origin\", \"id_destinity\", \"hora\")\n\n#Create a random route, it's will needed to initiate the algorithm\nroute_example = list(dict.fromkeys(cost_matrix_df).keys())\n\n#Instantiate the algorithm\nga = Genetic(p_m = 0.3, pop=400, gen=2000, k=5, p_c early_stoping=100,\n            max_time_work=6, extra_cost=12.83)\n\n#random route - time_matrix - cost_matrix\nresult = ga.fit(route_example, time_matrix, cost_matrix)\n\n#Printing the best route\nprint(result)\n\n#Printing the loss function\nprint(ga.history_)\n\n\n\nUnderstunding the data needed by the algorithm\n\n\nThe \ndicts of dicts\n time_cost and fuel_cost, are dicts of dicts for performance reason. these dicst could be also represented as diagonal symmetric matrix, columns and index represent the cost of the point. Here an example:\n\n\neasy_route = {\n            1:{\n                1:0,\n                2:x\n                3:y\n                },\n            2:{\n                1:x,\n                2:0,\n                3:z\n                },\n            3:{\n                1:y,\n                2:z,\n                3:0\n                }\n            }\n\n#We can see clearly the matrix with pandas\nimport pandas as pd\n\n#Visualice the diagonal symmetric matrix\nprint(pd.DataFrame(easy_route))\n\n\n\nOutput:\n\n\n\n\nIt's important to understund that the cost matrix \nmust be the computation of all the costs\n that the user of the algorithm want to take in count \nper person\n. In the majority of cases the cost will be composed by:\n\n\n\n\nThe cost of the fuel for one person to go from one point to another.\n\n\nThe cost of the hours of work needed from one point to another.\n\n\n\n\nIf we have multiple persons doing the same route, the \npeople\n parameter will multiply the cost by the number of persons and will increase the cost with the \nextra_cost\n in case of the \nmax_time_work\n paremeter will be surpased.\n\n\nAlso note that \nextra_cost\n \ncould be used as a maximun capacity\n of a truck for example in the case of material transports problems.\n\n\nThe other dict of dicts \ntime_matrix\n parameter will be only used by the algorithm to compute the \nmax_time_work\n and compute the final route of the algorithm with the times that the salesman/truck have finished their journey.\n\n\nEnsemble Genetic\n\n\n\n\nsklearn_route.genetics.EnsembleGenetic\n(n_genetics=10, p_c=0.6, p_m=0.4, pop=400, gen=1600, k=3, early_stopping=None,\n                max_time_work=8.0, extra_cost=10.0, people=1, n_jobs=1)\n\n\n\n\nEnsemble Genetic is a bagging of Genetics models, so you can refer to the documentation above to see how it works. Basically are a \nX\n number of Genetics estimator, this will help to get a better result than the Genetic Algorithm, the computationall cost will depend of the number of workers (n_jobs) you use. In the case you use the same \nn_jobs\n as \nn_genetics\n you will achieve the results in less than a second.\n\n\nAs said above, the algorith is pretty much the same, so here there are only the new hyper parameters.\n\n\nHyper - Parameters.\n\n\n\n\nn_genetics\n: int, default=10 \n\n\nThe number of Genetics algorithm that will be thrown. The more algorithms better result could be achieve, but's will be computationally more expensive (this can be solved/mitigated with the n_jobs parameter)\n\n\n\n\n\n\nn_jobs\n: int, default=1\n\n\nThe number of workers (threads) that will use the algorithm in parallel, by default is one, and must be at least one to the algorithm run. Use this parameter with caution, maybe can collapse the computer if you select a lot jobs. \n\n\n\n\n\n\n\n\nExample with Ensemble Genetic Algorithm\n\n\nfrom sklearn_route.datasets import load_valencia\nfrom sklearn_route.preprocessing import matrix_to_dict\nfrom sklearn_route.genetics import EnsembleGenetic\n\ndf_valencia = load_valencia()\n\n#Dataset - id of origin - id of destiny - column to transform (in this case the hour)\ntime_matrix = matrix_to_dict(df_valencia, \"id_origin\", \"id_destinity\", \"hora\")\n\n#Dataset - id of origin - id of destiny - column to transform (in this case the cost)\ncost_matrix = matrix_to_dict(df_valencia, \"id_origin\", \"id_destinity\", \"cost\")\n\n#Create a random route, it's will needed to initiate the algorithm\nroute_example = list(dict.fromkeys(cost_matrix_df).keys())\n\n#Instantiate the algorithm with 18 genetis algorithm and 6 jobs\neg = EnsembleGenetic(n_genetics=18, p_m = 0.4, pop=400, gen=2000, k=5, early_stoping=100,\n            max_time_work=6, extra_cost=12.83, n_jobs=6)\n\n#random route - time_matrix - cost_matrix\nresult = eg.fit(route_example, time_matrix, cost_matrix)\n\n#Printing the best route\nprint(result)",
            "title": "Genetics"
        },
        {
            "location": "/Genetics/#genetics",
            "text": "The main advantage of this Genetic algorithm over others solutions are:\n* Easy to learn and to use, inspired in Scikit Learn library.\n* Amazingly fast, builted in Cpython results with 400 populations and more than 1000 generations are done in less than a half of a second..\n* Very well documented.  Genetic Algorithm is a metaheuristic aproach wich mean is not deterministic, this lead that every time you run the result could be different but will be very near of the  global optimum .  This algorithm is called Genetic because is based in the Darwyn Theory of Evolution and the natural selection. This theory mainly says that  in a population of individuals, the best ones have more probabilities to survive than others. . This in our problems mean that we have one  initial random route  (individual), from this initialization we create a  list of  random routes  (random population), we are now in the  generation 0 . so the routes (individuals) of this population, as happend in reality, have the probability of mix one routes wich others and create new routes (have children), and this new routes will have mutations and crossover for each pair of routes (parents) (they have genetics modifications). So one time that all routes of the initial list of routes (random population) have pass through this phase (or not it's depend if they have childrens or not), we will finish the generation. when we finish the generation the worst route/s (individuals) will be replaced by the best route/s (this is the natural selection, the worst individuals die.) and will start the next generation with the new routes. And this will be repeated a number  X  of times (generations).",
            "title": "Genetics"
        },
        {
            "location": "/Genetics/#genetic",
            "text": "",
            "title": "Genetic"
        },
        {
            "location": "/Genetics/#hyper-parameters",
            "text": "sklearn_route.metaheuristics.genetics.Genetic (p_c=0.6, p_m=0.4, pop=400, gen=1600, k=3, early_stopping=None,\n                max_time_work=8.0, extra_cost=10.0, people=1)     p_c : float32, default=0.7   Between 0 and 1 the probability of crossover. In each generation\nat start the algorithm generate two random probabilities these random probabilies are the choosen when crossover will take place. If the random number is 4 that mean that the number in position and the following numbers will be now at start. And the first four numbers will be at the end. If this ratio is very low  (for example 0.1) will mean that a few new routes will be introduced (no childs in the population), so the optimization  will take more time, but the algorithm will be more stable . Instead  if it's too hight  (for example 0.9) will mean that the majority of the population will be changed per generation (new childs) this  will lead to a fast optimization but  will be more inestable so the algorithm  could not converge .     p_m : float32, default=0.4   It's mutate, random probabilies are choosen when it will take\nplace. If the random numbers are 2 and 8 for example, the \nnumbers located at that index will swap positions. \nThe same as croosover ( p_c ) a  higher numbers  (0.9)  will lead to  a lot of variation and a  faster optimization  in the routes (children)  but  also can make that the algorithm  could never converge. Instead  lower numbers  will lead to a  more stable algorithm but  the alogorithm  will be very slow  to converge.     pop : int32, default=400     The number of population (different random solutions), more\npopulation, more probabilities to find different solutions but \nmake the algorithm more time expensive. If the number is too\nlow, the solution will be bad, because the algorithm can't \niterate over the loss function with the enough amount of data     gen : int32 Default=1000   The number of generations. More generations make the algorithm\nachieve better solution but also make the algorithm take more\ntime. This can be solved with early_stoping parameter to make\nthe algorithm stop when is not improving. If the number of\ngeneration is low the algorithm may not converge.     k : int32, default=3   The number of individual populations (solutions) that\nwill fight for being the best. When we are inside of a\ngeneration the algorithm take a subsample of numbers k. The best\nk (individual) solution is choosen as a parten for this generation.     early_stopping : int32, default=None   If it's None, the algorithm will finish when the last generation \nis performed. If it's other unsigned integer (positive integer\nnumber) if the algorithm can't improve the result while X\ngenerations (where x is early_stopping number) the algorithm\nwill stop.     max_time_work : float32, default=8   The number of ours that a employ can work per day. For example\nif it's 8 hours, the algorithm will force that a route have to\nfinish after the 8 hours have been completed, making the\nemployeed come back home. it's a time constraint.     extra_cost : float32, default=0   If it's 0 anything happend. If it's > 0 in combination with\nmax_time_work when the max_time_work is reached, extra_cost is\napplied. This add a extra cost to the solution each time that\nmax_time_work is reached. It's like extra pay for the worker \neach time max_time_work is completed (journey).     people  int32, default=1   The number of people that you use in each route, for example if\nyou need two truck drivers. that's another contstraint. That\nwill multiply the time_costs and the extra_cost. Not the travel\ncost because it's assumed that both go in the same vehicle.",
            "title": "Hyper - Parameters"
        },
        {
            "location": "/Genetics/#method",
            "text": ".fit(route_example, time_costs, fuel_costs) :  This method train the algorithm, we need to pass the following data: \n        1.  route_example : Is a list random route to initiate the algorithm.\n        2.  time_cost : it's a dict of dicts that represent a diagonal matrix with the times between all points\n        3.  fuel_cost : it's a dict of dicts that represent a diagonal matrix with the costs between all points",
            "title": "Method"
        },
        {
            "location": "/Genetics/#attribute",
            "text": "history_ :  it's a list with the best cost in each generation. The loss function.",
            "title": "Attribute"
        },
        {
            "location": "/Genetics/#example-with-genetic-algorithm",
            "text": "from sklearn_route.datasets import load_barcelona\nfrom sklearn_route.preprocessing import matrix_to_dict\nfrom sklearn_route.metaheuristics.genetics import Genetic\n\ndf_barcelona = load_barcelona()\n\n#Dataset - id of origin - id of destiny - column to transform (in this case the hour)\ntime_matrix = matrix_to_dict(df_barcelona, \"id_origin\", \"id_destinity\", \"hora\")\n\n#Dataset - id of origin - id of destiny - column to transform (in this case the cost)\ncost_matrix = matrix_to_dict(df_barcelona, \"id_origin\", \"id_destinity\", \"hora\")\n\n#Create a random route, it's will needed to initiate the algorithm\nroute_example = list(dict.fromkeys(cost_matrix_df).keys())\n\n#Instantiate the algorithm\nga = Genetic(p_m = 0.3, pop=400, gen=2000, k=5, p_c early_stoping=100,\n            max_time_work=6, extra_cost=12.83)\n\n#random route - time_matrix - cost_matrix\nresult = ga.fit(route_example, time_matrix, cost_matrix)\n\n#Printing the best route\nprint(result)\n\n#Printing the loss function\nprint(ga.history_)",
            "title": "Example with Genetic Algorithm."
        },
        {
            "location": "/Genetics/#understunding-the-data-needed-by-the-algorithm",
            "text": "The  dicts of dicts  time_cost and fuel_cost, are dicts of dicts for performance reason. these dicst could be also represented as diagonal symmetric matrix, columns and index represent the cost of the point. Here an example:  easy_route = {\n            1:{\n                1:0,\n                2:x\n                3:y\n                },\n            2:{\n                1:x,\n                2:0,\n                3:z\n                },\n            3:{\n                1:y,\n                2:z,\n                3:0\n                }\n            }\n\n#We can see clearly the matrix with pandas\nimport pandas as pd\n\n#Visualice the diagonal symmetric matrix\nprint(pd.DataFrame(easy_route))  Output:   It's important to understund that the cost matrix  must be the computation of all the costs  that the user of the algorithm want to take in count  per person . In the majority of cases the cost will be composed by:   The cost of the fuel for one person to go from one point to another.  The cost of the hours of work needed from one point to another.   If we have multiple persons doing the same route, the  people  parameter will multiply the cost by the number of persons and will increase the cost with the  extra_cost  in case of the  max_time_work  paremeter will be surpased.  Also note that  extra_cost   could be used as a maximun capacity  of a truck for example in the case of material transports problems.  The other dict of dicts  time_matrix  parameter will be only used by the algorithm to compute the  max_time_work  and compute the final route of the algorithm with the times that the salesman/truck have finished their journey.",
            "title": "Understunding the data needed by the algorithm"
        },
        {
            "location": "/Genetics/#ensemble-genetic",
            "text": "sklearn_route.genetics.EnsembleGenetic (n_genetics=10, p_c=0.6, p_m=0.4, pop=400, gen=1600, k=3, early_stopping=None,\n                max_time_work=8.0, extra_cost=10.0, people=1, n_jobs=1)   Ensemble Genetic is a bagging of Genetics models, so you can refer to the documentation above to see how it works. Basically are a  X  number of Genetics estimator, this will help to get a better result than the Genetic Algorithm, the computationall cost will depend of the number of workers (n_jobs) you use. In the case you use the same  n_jobs  as  n_genetics  you will achieve the results in less than a second.  As said above, the algorith is pretty much the same, so here there are only the new hyper parameters.",
            "title": "Ensemble Genetic"
        },
        {
            "location": "/Genetics/#hyper-parameters_1",
            "text": "n_genetics : int, default=10   The number of Genetics algorithm that will be thrown. The more algorithms better result could be achieve, but's will be computationally more expensive (this can be solved/mitigated with the n_jobs parameter)    n_jobs : int, default=1  The number of workers (threads) that will use the algorithm in parallel, by default is one, and must be at least one to the algorithm run. Use this parameter with caution, maybe can collapse the computer if you select a lot jobs.",
            "title": "Hyper - Parameters."
        },
        {
            "location": "/Genetics/#example-with-ensemble-genetic-algorithm",
            "text": "from sklearn_route.datasets import load_valencia\nfrom sklearn_route.preprocessing import matrix_to_dict\nfrom sklearn_route.genetics import EnsembleGenetic\n\ndf_valencia = load_valencia()\n\n#Dataset - id of origin - id of destiny - column to transform (in this case the hour)\ntime_matrix = matrix_to_dict(df_valencia, \"id_origin\", \"id_destinity\", \"hora\")\n\n#Dataset - id of origin - id of destiny - column to transform (in this case the cost)\ncost_matrix = matrix_to_dict(df_valencia, \"id_origin\", \"id_destinity\", \"cost\")\n\n#Create a random route, it's will needed to initiate the algorithm\nroute_example = list(dict.fromkeys(cost_matrix_df).keys())\n\n#Instantiate the algorithm with 18 genetis algorithm and 6 jobs\neg = EnsembleGenetic(n_genetics=18, p_m = 0.4, pop=400, gen=2000, k=5, early_stoping=100,\n            max_time_work=6, extra_cost=12.83, n_jobs=6)\n\n#random route - time_matrix - cost_matrix\nresult = eg.fit(route_example, time_matrix, cost_matrix)\n\n#Printing the best route\nprint(result)",
            "title": "Example with Ensemble Genetic Algorithm"
        },
        {
            "location": "/Heuristics/",
            "text": "",
            "title": "Heuristics"
        },
        {
            "location": "/Installation/",
            "text": "",
            "title": "Installation"
        },
        {
            "location": "/MetaHeuristics/",
            "text": "",
            "title": "MetaHeuristics"
        },
        {
            "location": "/NRBS/",
            "text": "Node Ranking Based on Stats (NRBS)\n\n\nThe main advataje of NRBS over other solutions are:\n\n\n\n\nEasy to lean and to use inspired in Scikit Learn library\n\n\nHeuritic approach, wich mean it's deterministic, NRBS returns always the same solution with the same Hyperparameters\n\n\nVery Fast and with very good results\n\n\n\n\nGlobal understunding of the Algorithm\n\n\nNRBS is based on \nthis paper\n. Basically this algorithm base the \nconnections between points of a route in the mean and standar deviation of the distance\n from ones point each others. For that we have two formulas:\n\n\n\n\nPriority Formula: given the mean and the standar deviation of a point distance to the rest of points, Priority function raise them to an exponent and multiply both. The node with the highest priority is the first choosen to select a connection, and it's continue till the node with less priority\n\n\n\n\n\n\nP_i = \\mu_i^\\alpha \\sigma_i^\\beta\n\n\nP_i = \\mu_i^\\alpha \\sigma_i^\\beta\n\n\n\n\n\n\nConnection Formula: based on the mean, the standar deviation and the distance from one node to other, give us back the connection ratting, the nodes with highest connections are choosed to connect. The Exponents are the different hyper parametrs in this formula\n\n\n\n\n\n\n C_j = \\frac{\\mu_j^\\delta \\sigma_j^\\epsilon}{d_{j}^{i^\\lambda}}\n\n\n C_j = \\frac{\\mu_j^\\delta \\sigma_j^\\epsilon}{d_{j}^{i^\\lambda}}\n\n\n\n\nCaveat:\n This alorithm solve the TSP/VRP problem, by now it's not available with constraints but it will be in the future. \n\n\nNRBS Hyper Parameters\n\n\n\n\nsklearn_route.genetics.Genetic\n(p_c=0.6, p_m=0.4, pop=400, gen=1600, k=3, early_stopping=None,\n                max_time_work=8.0, extra_cost=10.0, people=1)\n\n\n\n\n\n\n\n\nmean_priority\n: float\n\n\n\n\nMust be a number greater than zero, it is the exponent of the Priority formula mean, higher number make points with higher mean (more distance from others) have large values and priority selection, strongly recommended a number between 0 and 2.\n\n\n\n\n\n\n\n\nstd_priority\n: float\n\n\n\n\nMust be a number greater than zero, it is the exponent of the Priority formula standar deviation, higher number make points with higher deviation (more diferents of distance between points) have larger values and priority selection, Strongly recommended a number between 0 and 2\n\n\n\n\n\n\n\n\nmean_connection\n: float\n\n\n\n\nMust be a number greater than zero, it is the exponent of the Connection formula mean, higher number make points with higher mean (more distance from others) have large values and priority connection, strongly recommended a number between 0 and 2.\n\n\n\n\n\n\n\n\nstd_connection\n: float\n\n\n\n\nMust be a number greater than zero, it is the exponent of the Connections formula standar deviation, higher number make points with higher deviation (more diferents of distance between points) have larger values and priority connection, Strongly recommended a number between 0 and 2\n\n\n\n\n\n\n\n\ndistance_weight\n: float\n\n\n\n\nMust be a number greater than zero, it is the exponent of the Connections formula standar deviation, higher number make points with higher mean and deviation haves less importance and lower ratting at connection  have larger Strongly recommended a number between 0 and 2\n\n\n\n\n\n\n\n\nMethod\n\n\n.fit(route_example, time_costs, fuel_costs):\n\n\n\n\nThis method train the algorithm, we need to pass the following parameters:\n\n\nstart_node_id\n: The starting node.\n\n\nids_node\n: a list with all the nodes ids. \n\n\ncost_matrix\n: a dict of dicts that represent a diagonal matrix with the times between all points. \n\n\n\n\nExample with NRBS algorithm\n\n\nfrom sklearn_route.datasets import load_barcelona\nfrom sklearn_route.preprocessing import matrix_to_dict\nfrom sklearn_route.heuristics import NRBS \n\ndf_barcelona = load_barcelona()\n\n#Dataset - id of origin - id of destiny - column to transform (in this case the cost)\ncost_matrix = matrix_to_dict(df_barcelona, \"id_origin\", \"id_destinity\", \"hora\")\n\n#Create a random route, it's will needed to initiate the algorithm\nroute_example = list(dict.fromkeys(cost_matrix_df).keys())\nstart_id = route_example[0]\n\nnrbs = NRBS(mean_priority=0.3, pop=1.5, gen=0.4, k=2, p_c early_stoping=1)\n\n#random route - time_matrix - cost_matrix\nresult = nrbs.fit(start_id, route_example, cost_matrix)\n\n#Print the cost and the route.\nprint(result)\n\n\n\nUnderstunding the data needed by the algorithm\n\n\nThe \ndicts of dicts\n matrix_cost, are dicts of dicts for performance reason. these dicts could be also represented as diagonal symmetric matrix, columns and index represent the cost of the point. Here an example:\n\n\neasy_route = {\n            1:{\n                1:0,\n                2:x\n                3:y\n                },\n            2:{\n                1:x,\n                2:0,\n                3:z\n                },\n            3:{\n                1:y,\n                2:z,\n                3:0\n                }\n            }\n\n#We can see clearly the matrix with pandas\nimport pandas as pd\n\n#Visualice the diagonal symmetric matrix\nprint(pd.DataFrame(easy_route))\n\n\n\nOutput:\n\n\n\n\nIt's important to understund that the cost matrix \nmust be the computation of all the costs\n that the user of the algorithm want to take in count \nper person\n. In the majority of cases the cost will be composed by:\n\n\n\n\nThe cost of the fuel for one person to go from one point to another.\n\n\nThe cost of the hours of work needed from one point to another.\n\n\n\n\nIf we have multiple persons doing the same route, the \npeople\n parameter will multiply the cost by the number of persons and will increase the cost with the \nextra_cost\n in case of the \nmax_time_work\n paremeter will be surpased.\n\n\nAlso note that \nextra_cost\n \ncould be used as a maximun capacity\n of a truck for example in the case of material transports problems.",
            "title": "NRBS"
        },
        {
            "location": "/NRBS/#node-ranking-based-on-stats-nrbs",
            "text": "The main advataje of NRBS over other solutions are:   Easy to lean and to use inspired in Scikit Learn library  Heuritic approach, wich mean it's deterministic, NRBS returns always the same solution with the same Hyperparameters  Very Fast and with very good results",
            "title": "Node Ranking Based on Stats (NRBS)"
        },
        {
            "location": "/NRBS/#global-understunding-of-the-algorithm",
            "text": "NRBS is based on  this paper . Basically this algorithm base the  connections between points of a route in the mean and standar deviation of the distance  from ones point each others. For that we have two formulas:   Priority Formula: given the mean and the standar deviation of a point distance to the rest of points, Priority function raise them to an exponent and multiply both. The node with the highest priority is the first choosen to select a connection, and it's continue till the node with less priority    P_i = \\mu_i^\\alpha \\sigma_i^\\beta  P_i = \\mu_i^\\alpha \\sigma_i^\\beta    Connection Formula: based on the mean, the standar deviation and the distance from one node to other, give us back the connection ratting, the nodes with highest connections are choosed to connect. The Exponents are the different hyper parametrs in this formula     C_j = \\frac{\\mu_j^\\delta \\sigma_j^\\epsilon}{d_{j}^{i^\\lambda}}   C_j = \\frac{\\mu_j^\\delta \\sigma_j^\\epsilon}{d_{j}^{i^\\lambda}}   Caveat:  This alorithm solve the TSP/VRP problem, by now it's not available with constraints but it will be in the future.",
            "title": "Global understunding of the Algorithm"
        },
        {
            "location": "/NRBS/#nrbs-hyper-parameters",
            "text": "sklearn_route.genetics.Genetic (p_c=0.6, p_m=0.4, pop=400, gen=1600, k=3, early_stopping=None,\n                max_time_work=8.0, extra_cost=10.0, people=1)     mean_priority : float   Must be a number greater than zero, it is the exponent of the Priority formula mean, higher number make points with higher mean (more distance from others) have large values and priority selection, strongly recommended a number between 0 and 2.     std_priority : float   Must be a number greater than zero, it is the exponent of the Priority formula standar deviation, higher number make points with higher deviation (more diferents of distance between points) have larger values and priority selection, Strongly recommended a number between 0 and 2     mean_connection : float   Must be a number greater than zero, it is the exponent of the Connection formula mean, higher number make points with higher mean (more distance from others) have large values and priority connection, strongly recommended a number between 0 and 2.     std_connection : float   Must be a number greater than zero, it is the exponent of the Connections formula standar deviation, higher number make points with higher deviation (more diferents of distance between points) have larger values and priority connection, Strongly recommended a number between 0 and 2     distance_weight : float   Must be a number greater than zero, it is the exponent of the Connections formula standar deviation, higher number make points with higher mean and deviation haves less importance and lower ratting at connection  have larger Strongly recommended a number between 0 and 2",
            "title": "NRBS Hyper Parameters"
        },
        {
            "location": "/NRBS/#method",
            "text": ".fit(route_example, time_costs, fuel_costs):   This method train the algorithm, we need to pass the following parameters:  start_node_id : The starting node.  ids_node : a list with all the nodes ids.   cost_matrix : a dict of dicts that represent a diagonal matrix with the times between all points.",
            "title": "Method"
        },
        {
            "location": "/NRBS/#example-with-nrbs-algorithm",
            "text": "from sklearn_route.datasets import load_barcelona\nfrom sklearn_route.preprocessing import matrix_to_dict\nfrom sklearn_route.heuristics import NRBS \n\ndf_barcelona = load_barcelona()\n\n#Dataset - id of origin - id of destiny - column to transform (in this case the cost)\ncost_matrix = matrix_to_dict(df_barcelona, \"id_origin\", \"id_destinity\", \"hora\")\n\n#Create a random route, it's will needed to initiate the algorithm\nroute_example = list(dict.fromkeys(cost_matrix_df).keys())\nstart_id = route_example[0]\n\nnrbs = NRBS(mean_priority=0.3, pop=1.5, gen=0.4, k=2, p_c early_stoping=1)\n\n#random route - time_matrix - cost_matrix\nresult = nrbs.fit(start_id, route_example, cost_matrix)\n\n#Print the cost and the route.\nprint(result)",
            "title": "Example with NRBS algorithm"
        },
        {
            "location": "/NRBS/#understunding-the-data-needed-by-the-algorithm",
            "text": "The  dicts of dicts  matrix_cost, are dicts of dicts for performance reason. these dicts could be also represented as diagonal symmetric matrix, columns and index represent the cost of the point. Here an example:  easy_route = {\n            1:{\n                1:0,\n                2:x\n                3:y\n                },\n            2:{\n                1:x,\n                2:0,\n                3:z\n                },\n            3:{\n                1:y,\n                2:z,\n                3:0\n                }\n            }\n\n#We can see clearly the matrix with pandas\nimport pandas as pd\n\n#Visualice the diagonal symmetric matrix\nprint(pd.DataFrame(easy_route))  Output:   It's important to understund that the cost matrix  must be the computation of all the costs  that the user of the algorithm want to take in count  per person . In the majority of cases the cost will be composed by:   The cost of the fuel for one person to go from one point to another.  The cost of the hours of work needed from one point to another.   If we have multiple persons doing the same route, the  people  parameter will multiply the cost by the number of persons and will increase the cost with the  extra_cost  in case of the  max_time_work  paremeter will be surpased.  Also note that  extra_cost   could be used as a maximun capacity  of a truck for example in the case of material transports problems.",
            "title": "Understunding the data needed by the algorithm"
        },
        {
            "location": "/Preprocessing/",
            "text": "Preprocessing\n\n\nThe preprocessing module could be confused if it's compared with the Scikit Learn Preprocessing module. In this case this module is not a Mathematical module for preprocesing data, it's a module oriente to preprocesed data in order to feed the algorithms, wich means that it's not make it with mathematical purposes only with structure purposes.\n\n\nThe different functions of the preprocessing module are:\n\n\nmatrix_to_dict\n\n\nmatrix_to_dict\n: is a function that convert a diagonal matrix with origins -> destiny points from a dataframe to a dicts of dicts. This dict of dict is needed for the \nGenetic\n algorithm and the \nEnsembleGenetic\n algorithm in order to compute their results. The parameters of the function are:\n\n\n\n\ndataframe\n: the dataframe that will be converted\n\n\nid_origin\n: the name of the column with the id of the origin points \n\n\nid_destinity\n: the name of the column with the id of the destinity points\n\n\ncompute_column\n: the column data that will be taken in order to create the data of the diagonal matrix\n\n\n\n\nWe have the following Dataframe (Barceloa dataframe from datasets):\n\n\n\nUse example:\n\n\nfrom sklearn_route.datasets import load_barcelona\nfrom sklearn_route.preprocessing import matrix_to_dict\n\n#Dataset - id of origin - id of destiny - column to transform (in this case the hour)\ntime_matrix = matrix_to_dict(df_barcelona, \"id_origin\", \"id_destinity\", \"hora\")\n\n#Dataset - id of origin - id of destiny - column to transform (in this case the cost)\ncost_matrix = matrix_to_dict(df_barcelona, \"id_origin\", \"id_destinity\", \"hora\")\n\n\n\nSo the dict of dicts represent a diagonal matrix. it's transformed in a dict of dicts for performance reasons. Only for learning purpose we will represent the dict of dicts as a diagonal matrix:\n\n\nimport pandas as pd\n\ndf = pd.DataFrame(cost_matrix_df)\n\n\n\nAnd the result:\n\n\n\n\nThis matrix represent the cost of go from one point (origin id) to other (destinity id). This cost is computed with the hours and the cost of fuel per kilometer.",
            "title": "Preprocessing"
        },
        {
            "location": "/Preprocessing/#preprocessing",
            "text": "The preprocessing module could be confused if it's compared with the Scikit Learn Preprocessing module. In this case this module is not a Mathematical module for preprocesing data, it's a module oriente to preprocesed data in order to feed the algorithms, wich means that it's not make it with mathematical purposes only with structure purposes.  The different functions of the preprocessing module are:",
            "title": "Preprocessing"
        },
        {
            "location": "/Preprocessing/#matrix_to_dict",
            "text": "matrix_to_dict : is a function that convert a diagonal matrix with origins -> destiny points from a dataframe to a dicts of dicts. This dict of dict is needed for the  Genetic  algorithm and the  EnsembleGenetic  algorithm in order to compute their results. The parameters of the function are:   dataframe : the dataframe that will be converted  id_origin : the name of the column with the id of the origin points   id_destinity : the name of the column with the id of the destinity points  compute_column : the column data that will be taken in order to create the data of the diagonal matrix   We have the following Dataframe (Barceloa dataframe from datasets):  Use example:  from sklearn_route.datasets import load_barcelona\nfrom sklearn_route.preprocessing import matrix_to_dict\n\n#Dataset - id of origin - id of destiny - column to transform (in this case the hour)\ntime_matrix = matrix_to_dict(df_barcelona, \"id_origin\", \"id_destinity\", \"hora\")\n\n#Dataset - id of origin - id of destiny - column to transform (in this case the cost)\ncost_matrix = matrix_to_dict(df_barcelona, \"id_origin\", \"id_destinity\", \"hora\")  So the dict of dicts represent a diagonal matrix. it's transformed in a dict of dicts for performance reasons. Only for learning purpose we will represent the dict of dicts as a diagonal matrix:  import pandas as pd\n\ndf = pd.DataFrame(cost_matrix_df)  And the result:   This matrix represent the cost of go from one point (origin id) to other (destinity id). This cost is computed with the hours and the cost of fuel per kilometer.",
            "title": "matrix_to_dict"
        },
        {
            "location": "/SOM/",
            "text": "Self Organizing Maps (SOM)\n\n\nCan be used to solve de TSP/VRP problem\n\n\nThe main \nadvantages\n of SOM over others solutions are:\n* Very good results even with a huge amount of points\n* It's computed using only the point ID, latitude and longitude\n\n\nThe main \ndisvantages\n of SOM over other solutions are:\n*  The result is a very optimized route but you don't know the cost\n*  Computationally more expensive than other solutions\n*  Constrains (time/capacity) are not available in this algorithm \n*  Penalties are not available in this algorithm (time/capacity)\n\n\nGlobal understunding of the Algorithm\n\n\nSOM is metaheuristic aproach wich mean is not deterministic, this lead that every time you run the algorithm you cound find a different solution.\n\n\nThis algorithm is based on Self Organizing Maps from \nTehuvo Kohonen\n and the implementation in Python is based on the \nDiego Vicente\n solution.  \n\n\nThe main formula of the alorithm if the search formula, to explore new combinations, combined with a learning rate decay in order to minimize the searchs over the iterations. Our algorithm can be expressed as:\n\n\n\n\nnt+1=nt+\u03b1t\u22c5g(we,ht)\u22c5\u0394(e,nt)\n\n\nnt+1=nt+\u03b1t\u22c5g(we,ht)\u22c5\u0394(e,nt)\n\n\n\n\nWhere \n\\delta\n\\delta\n is the learning rate, \ng\ng\n is the gaussian function that look for a winner in a radius of \nh\nh\n \n\n\nSOM\n\n\nHyper - Parameters\n\n\n\n\nsklearn_route.metaheuristics.som.SOM\n(units=None, radius=None, radius_decay=0.9991, lr=0.8, lr_decay=0.9991)\n\n\n\n\n\n\nunits\n: int, default=None\n\n\nThe number of Neurons of the SOM, if it's None the algorithm takes the number of nodes multiply by eight. More Numbers of neuron, better results but with a time penalty\n\n\n\n\n\n\nradius\n: int, defaultNone\n\n\nThe radius of search, if it's None the algorithm takes the number of nodes multiply by eight. The radius will decrease with the radius_decay parameter so it's good to have a high\nradius at start to find throug all nodes at the begining\n\n\n\n\n\n\nradius_decay\n: float, default=0.9991\n\n\nThe decay of the radius per epoch wich means the decrease of the radius\n\n\n\n\n\n\n\n\nlr\n: float, default=0.8\n\n\n\n\nThe learning rate, is how aggressive the update of the weight is, higher learning rate, more\n    aggressive is the update of the weights\n\n\n\n\n\n\n\n\nlr_decay\n: float, default=0.9991\n\n\n\n\nThe decay of the learning rate. In order to find the most optimal solution, at the end is good to have a low learning rate.\n\n\n\n\n\n\n\n\nMethod\n\n\n\n\n\n\nfit(nodes, epochs)\n:\n\n\n\n\nExecute the algorithm and give back the best route find it\n\n\n\n\n\n\n\n\nnodes\n: tuple\n\n\n\n\nA tuple of tuples, each tuple is a Node with the first element the ID, the sencond the latitude\n    and the third then longitude. For example if we have a route with three points, the tuple will\n    be like this:\n\n\n\n\nnodes = (\n\n    (1, 0.459887, 14.345767),\n\n    (2, 0.634534, 12.575462),\n\n    (3, 0.256765, 9.734435),\n\n)\n\n    \n\n\n\n\n\n\nepochs\n: int, default=10_000\n\n\n\n\nThe times that the Neural network will update the weights trying to find the optimal solution\n\n\n\n\n\n\n\n\n\n\n\n\nExample SOM\n\n\nfrom sklearn_route.datasets import load_barcelona\nfrom sklearn_route.preprocessing import normalize, df_to_tuple\nfrom sklearn_route.metaheuristics.som import SOM\n\ndf_barcelona = load_barcelona()[\"DataFrame\"]\ndf_barcelona = df_barcelona[[\"id_origin\", \"lat_origin\", \"lon_origin\"]].drop_duplicates()\ndf_barcelona = normalize(df_barcelona, \"lat_origin\", \"lon_origin\")\n\nroute = df_to_tuple(df_Barcelona)\n\nsom = SOM()\nresult = som.fit(route)\n\n#Printing the best route\nprint(result)",
            "title": "SOM"
        },
        {
            "location": "/SOM/#self-organizing-maps-som",
            "text": "Can be used to solve de TSP/VRP problem  The main  advantages  of SOM over others solutions are:\n* Very good results even with a huge amount of points\n* It's computed using only the point ID, latitude and longitude  The main  disvantages  of SOM over other solutions are:\n*  The result is a very optimized route but you don't know the cost\n*  Computationally more expensive than other solutions\n*  Constrains (time/capacity) are not available in this algorithm \n*  Penalties are not available in this algorithm (time/capacity)",
            "title": "Self Organizing Maps (SOM)"
        },
        {
            "location": "/SOM/#global-understunding-of-the-algorithm",
            "text": "SOM is metaheuristic aproach wich mean is not deterministic, this lead that every time you run the algorithm you cound find a different solution.  This algorithm is based on Self Organizing Maps from  Tehuvo Kohonen  and the implementation in Python is based on the  Diego Vicente  solution.    The main formula of the alorithm if the search formula, to explore new combinations, combined with a learning rate decay in order to minimize the searchs over the iterations. Our algorithm can be expressed as:   nt+1=nt+\u03b1t\u22c5g(we,ht)\u22c5\u0394(e,nt)  nt+1=nt+\u03b1t\u22c5g(we,ht)\u22c5\u0394(e,nt)   Where  \\delta \\delta  is the learning rate,  g g  is the gaussian function that look for a winner in a radius of  h h",
            "title": "Global understunding of the Algorithm"
        },
        {
            "location": "/SOM/#som",
            "text": "",
            "title": "SOM"
        },
        {
            "location": "/SOM/#hyper-parameters",
            "text": "sklearn_route.metaheuristics.som.SOM (units=None, radius=None, radius_decay=0.9991, lr=0.8, lr_decay=0.9991)    units : int, default=None  The number of Neurons of the SOM, if it's None the algorithm takes the number of nodes multiply by eight. More Numbers of neuron, better results but with a time penalty    radius : int, defaultNone  The radius of search, if it's None the algorithm takes the number of nodes multiply by eight. The radius will decrease with the radius_decay parameter so it's good to have a high\nradius at start to find throug all nodes at the begining    radius_decay : float, default=0.9991  The decay of the radius per epoch wich means the decrease of the radius     lr : float, default=0.8   The learning rate, is how aggressive the update of the weight is, higher learning rate, more\n    aggressive is the update of the weights     lr_decay : float, default=0.9991   The decay of the learning rate. In order to find the most optimal solution, at the end is good to have a low learning rate.",
            "title": "Hyper - Parameters"
        },
        {
            "location": "/SOM/#method",
            "text": "fit(nodes, epochs) :   Execute the algorithm and give back the best route find it     nodes : tuple   A tuple of tuples, each tuple is a Node with the first element the ID, the sencond the latitude\n    and the third then longitude. For example if we have a route with three points, the tuple will\n    be like this:   nodes = ( \n    (1, 0.459887, 14.345767), \n    (2, 0.634534, 12.575462), \n    (3, 0.256765, 9.734435), \n) \n        epochs : int, default=10_000   The times that the Neural network will update the weights trying to find the optimal solution",
            "title": "Method"
        },
        {
            "location": "/SOM/#example-som",
            "text": "from sklearn_route.datasets import load_barcelona\nfrom sklearn_route.preprocessing import normalize, df_to_tuple\nfrom sklearn_route.metaheuristics.som import SOM\n\ndf_barcelona = load_barcelona()[\"DataFrame\"]\ndf_barcelona = df_barcelona[[\"id_origin\", \"lat_origin\", \"lon_origin\"]].drop_duplicates()\ndf_barcelona = normalize(df_barcelona, \"lat_origin\", \"lon_origin\")\n\nroute = df_to_tuple(df_Barcelona)\n\nsom = SOM()\nresult = som.fit(route)\n\n#Printing the best route\nprint(result)",
            "title": "Example SOM"
        },
        {
            "location": "/Simmulated_Annealing/",
            "text": "Simulated Annealing\n\n\nThe main advantage of Simulated Annealing over others metaheuristics algorithms is his speed. It can be instantiate and solved in less than a tenth of a second, that give the oportunity to make a lot of search in the loss function to improve the results. Also as other Algorithms of this library it's based on Scikit Learn so is very easy to use.\n\n\nGlobal understunding of the Algorithm\n\n\nGenetic Algorithm is a metaheuristic aproach wich mean is not deterministic, this lead that every time you run the result could be different but will be very near of the global optimum.\n\n\nThis algorith is used on the metal cooling to decrease the \ntemperature\n to the optimal number. The temperature is decresed by a \ndelta\n parameter, each time the temperature decrease by the delta parameter, th e algorithm looks for a number \nneighbours\n of combination of routes. When the temperature decrease to the minimun temperature \ntolerated\n the algorithms stop\n\n\ntolerance\n it's like iterations, given a temperature \nX\n, more tolerance, less iterations, less tolerance, moree iterations.\n\n\nSimmulated annealing\n\n\n\n\nsklearn_route.simulated_annealing.SimulatedAnnealing\n(temp=12.0, neighbours=250, delta=0.78, tol=1.29,\n            max_time_work=8.0, extra_cost=10.0, people=1)\n\n\n\n\nHyper -  Parameters\n\n\n\n\n\n\ntemp\n: float32, default=12\n\n\n\n\nThe temperature parameter. In the algorithm the temperature will \ndecrease till reach the tol parameter to give you the best\nsolution. If the temp is hight the algorithm is slower and \nlook for more posible solutions if it's low the algorithm it's\nfaster but look on less solutions. Note that is not always good\nfor the optimization look more solutions, because this parameter\nis combined with delta and tol.\n\n\n\n\n\n\n\n\nneighbours\n: int32 , default=250\n\n\n\n\nGiven a route the exchanges that will make in at the same temperature.\nYou can think as the neighbours that will visit at a particular\ntemperature. If the \nneighbours\n is hight the algorithm is slower and \nlook for more posible solutions if it's low the algorithm it's\nfaster but look on less solutions. If it's too hight you probably will\ntry the sames solutions over and over again.\n\n\n\n\n\n\n\n\ndelta\n: float32, default=0.78\n\n\n\n\nA number between 0 and 1 the highest the number more slow the\nthe temperature decrease, and more solution the algorithm try.\nIf it's close to 0 the temperature decrease fast and the\nalgorithm converge faster but look for less optimal solutions\n\n\n\n\n\n\n\n\ntol\n: float32, default=1.29\n\n\n\n\nThe tolerance, the minimun temperature allow by the algorithm\none time the \"temp\" is equal to \"tol\" the algorithm finish.\n\n\n\n\n\n\n\n\nmax_time_work\n: float32, default=8\n\n\n\n\nThe number of ours that a employ can work per day. For example\nif it's 8 hours, the algorithm will force that a route have to\nfinish after the 8 hours have been completed, making the\nemployeed come back home. it's a time constraint.\n\n\n\n\n\n\n\n\nextra_cost\n: float32, default=0\n\n\n\n\nIf it's 0 anything happend. If it's > 0 in combination with\nmax_time_work when the max_time_work is reached, extra_cost is\napplied. This add a extra cost to the solution each time that\nmax_time_work is reached. It's like extra pay for the worker \neach time max_time_work is completed (journey).\n\n\n\n\n\n\n\n\npeople\n int32, default=1\n\n\n\n\nThe number of people that you use in each route, for example if\nyou need two truck drivers. that's another contstraint. That\nwill multiply the time_costs and the extra_cost. Not the travel\ncost because it's assumed that both go in the same vehicle.\n\n\n\n\n\n\n\n\nMethod\n\n\n\n\n.fit(route_example, time_costs, fuel_costs)\n:\n\n\nThis method train the algorithm, we need to pass the following data:\n\n        1. \nroute_example\n: Is a list random route to initiate the algorithm.\n        2. \ntime_cost\n: it's a dict of dicts that represent a diagonal matrix with the times between all points\n        3. \nfuel_cost\n: it's a dict of dicts that represent a diagonal matrix with the costs between all points\n\n\n\n\n\n\n\n\nAttribute\n\n\n\n\nhistory_\n:\n\n\nit's a list with the best cost in each generation. The loss function.\n\n\n\n\n\n\n\n\nExamples with Simulated Annealing.\n\n\nfrom sklearn_route.datasets import load_barcelona\nfrom sklearn_route.preprocessing import matrix_to_dict\nfrom sklearn_route.metaheuristics.simulated_annealing import SimulatedAnnealing\n\ndf_barcelona = load_barcelona()\n\n#Dataset - id of origin - id of destiny - column to transform (in this case the hour)\ntime_matrix = matrix_to_dict(df_barcelona, \"id_origin\", \"id_destinity\", \"hora\")\n\n#Dataset - id of origin - id of destiny - column to transform (in this case the cost)\ncost_matrix = matrix_to_dict(df_barcelona, \"id_origin\", \"id_destinity\", \"hora\")\n\n#Create a random route, it's will needed to initiate the algorithm\nroute_example = list(dict.fromkeys(cost_matrix_df).keys())\n\n#Instantiate the algorithm\nsa = SimulatedAnnealing(temp=10, neighbours=300, delta=0.82, tol=0.2,\n                        max_time_work=8, extra_cost=12.83, people=2)\n\n#random route - time_matrix - cost_matrix\nresult = sa.fit(route_example, time_matrix, cost_matrix)\n\n#Printing the best route\nprint(result)\n\n#Printing the loss function\nprint(sa.history_)\n\n\n\nUnderstunding the data needed by the algorithm\n\n\nThe \ndicts of dicts\n time_cost and fuel_cost, are dicts of dicts for performance reason. these dicst could be also represented as diagonal symmetric matrix, columns and index represent the cost of the point. Here an example:\n\n\neasy_route = {\n            1:{\n                1:0,\n                2:x\n                3:y\n                },\n            2:{\n                1:x,\n                2:0,\n                3:z\n                },\n            3:{\n                1:y,\n                2:z,\n                3:0\n                }\n            }\n\n#We can see clearly the matrix with pandas\nimport pandas as pd\n\n#Visualice the diagonal symmetric matrix\nprint(pd.DataFrame(easy_route))\n\n\n\nOutput:\n\n\n\n\nIt's important to understund that the cost matrix \nmust be the computation of all the costs\n that the user of the algorithm want to take in count \nper person\n. In the majority of cases the cost will be composed by:\n\n\n\n\nThe cost of the fuel for one person to go from one point to another.\n\n\nThe cost of the hours of work needed from one point to another.\n\n\n\n\nIf we have multiple persons doing the same route, the \npeople\n parameter will multiply the cost by the number of persons and will increase the cost with the \nextra_cost\n in case of the \nmax_time_work\n paremeter will be surpased.\n\n\nAlso note that \nextra_cost\n \ncould be used as a maximun capacity\n of a truck for example in the case of material transports problems.\n\n\nThe other dict of dicts \ntime_matrix\n parameter will be only used by the algorithm to compute the \nmax_time_work\n and compute the final route of the algorithm with the times that the salesman/truck have finished their journey.\n\n\nCaveats for beginners\n\n\nIt's always common for beginners think \"If I increase the time, with high \ntemp\n, high \nneighbours\n and low \ndelta\n \nthe results will be better\" This is not true, you have to find the optimal parameters for your especific problem.\nFor example if your data is compose by 4 differents places to visit, don't have sense to have 250 neighbours, because\nyou don't have a lot of differents combinations possible.\n\n\nOther case it's if you have for example a too high temperature, and the places are very near one each other then\nprobably the algorithm will never converge because hight temperature will lead to not accept worst solutions when\nthe algorithm is converging.\n\n\nEnsemble Simulated Annealing\n\n\n\n\nsklearn_route.simulated_annealing.EnsembleSimulatedAnnealing\n(n_simulateds=20,  temp=12.0, neighbours=250, delta=0.78, tol=1.29,\n            max_time_work=8.0, extra_cost=10.0, people=1, n_jobs=1)\n\n\n\n\nEnsemble Simulated Annealing is a bagging of Simulated Annealing models, so you can refer to the documentation above to see how it works. Basically are a \nX\n number of Genetics estimator, this help to get a better result than the Simulated Annealing alogorithm, the computationall cost will depend of the number of workers (n_jobs) you use. In the case you use the same \nn_jobs\n as \nn_simulateds\n you will be as fast in time as Simulated Anealing\n\n\nAs said above, the algorith is pretty much the same, so here there are only the new hyper parameters.\n\n\nHyper - Parameters.\n\n\n\n\nn_simulateds\n: int, default=10 \n\n\nThe number of Simulated Annealing algorithms that will be thrown. The more algorithms better result could be achieve, but's will be computationally more expensive (this can be solved/mitigated with the n_jobs parameter)\n\n\n\n\n\n\nn_jobs\n: int, default=1\n\n\nThe number of workers (threads) that will use the algorithm in parallel, by default is one, and must be at least one to the algorithm run. Use this parameter with caution, maybe can collapse the computer if you select a lot jobs. \n\n\n\n\n\n\n\n\nExample with Ensemble Simulated Annealing\n\n\nfrom sklearn_route.datasets import load_valencia\nfrom sklearn_route.preprocessing import matrix_to_dict\nfrom sklearn_route.genetics import EnsembleGenetic\n\ndf_valencia = load_valencia()\n\n#Dataset - id of origin - id of destiny - column to transform (in this case the hour)\ntime_matrix = matrix_to_dict(df_valencia, \"id_origin\", \"id_destinity\", \"hora\")\n\n#Dataset - id of origin - id of destiny - column to transform (in this case the cost)\ncost_matrix = matrix_to_dict(df_valencia, \"id_origin\", \"id_destinity\", \"cost\")\n\n#Create a random route, it's will needed to initiate the algorithm\nroute_example = list(dict.fromkeys(cost_matrix_df).keys())\n\n#Instantiate the algorithm with 18 simulated annealing algorithm and 6 jobs\nesa = EnsembleSimulatedAnneling(n_simulateds=18, temp = 18, neighbours=300, max_time_work=6, extra_cost=12.83, n_jobs=6)\n\n#random route - time_matrix - cost_matrix\nresult = esa.fit(route_example, time_matrix, cost_matrix)\n\n#Printing the best route\nprint(result)",
            "title": "Simmulated Annealing"
        },
        {
            "location": "/Simmulated_Annealing/#simulated-annealing",
            "text": "The main advantage of Simulated Annealing over others metaheuristics algorithms is his speed. It can be instantiate and solved in less than a tenth of a second, that give the oportunity to make a lot of search in the loss function to improve the results. Also as other Algorithms of this library it's based on Scikit Learn so is very easy to use.",
            "title": "Simulated Annealing"
        },
        {
            "location": "/Simmulated_Annealing/#global-understunding-of-the-algorithm",
            "text": "Genetic Algorithm is a metaheuristic aproach wich mean is not deterministic, this lead that every time you run the result could be different but will be very near of the global optimum.  This algorith is used on the metal cooling to decrease the  temperature  to the optimal number. The temperature is decresed by a  delta  parameter, each time the temperature decrease by the delta parameter, th e algorithm looks for a number  neighbours  of combination of routes. When the temperature decrease to the minimun temperature  tolerated  the algorithms stop  tolerance  it's like iterations, given a temperature  X , more tolerance, less iterations, less tolerance, moree iterations.",
            "title": "Global understunding of the Algorithm"
        },
        {
            "location": "/Simmulated_Annealing/#simmulated-annealing",
            "text": "sklearn_route.simulated_annealing.SimulatedAnnealing (temp=12.0, neighbours=250, delta=0.78, tol=1.29,\n            max_time_work=8.0, extra_cost=10.0, people=1)",
            "title": "Simmulated annealing"
        },
        {
            "location": "/Simmulated_Annealing/#hyper-parameters",
            "text": "temp : float32, default=12   The temperature parameter. In the algorithm the temperature will \ndecrease till reach the tol parameter to give you the best\nsolution. If the temp is hight the algorithm is slower and \nlook for more posible solutions if it's low the algorithm it's\nfaster but look on less solutions. Note that is not always good\nfor the optimization look more solutions, because this parameter\nis combined with delta and tol.     neighbours : int32 , default=250   Given a route the exchanges that will make in at the same temperature.\nYou can think as the neighbours that will visit at a particular\ntemperature. If the  neighbours  is hight the algorithm is slower and \nlook for more posible solutions if it's low the algorithm it's\nfaster but look on less solutions. If it's too hight you probably will\ntry the sames solutions over and over again.     delta : float32, default=0.78   A number between 0 and 1 the highest the number more slow the\nthe temperature decrease, and more solution the algorithm try.\nIf it's close to 0 the temperature decrease fast and the\nalgorithm converge faster but look for less optimal solutions     tol : float32, default=1.29   The tolerance, the minimun temperature allow by the algorithm\none time the \"temp\" is equal to \"tol\" the algorithm finish.     max_time_work : float32, default=8   The number of ours that a employ can work per day. For example\nif it's 8 hours, the algorithm will force that a route have to\nfinish after the 8 hours have been completed, making the\nemployeed come back home. it's a time constraint.     extra_cost : float32, default=0   If it's 0 anything happend. If it's > 0 in combination with\nmax_time_work when the max_time_work is reached, extra_cost is\napplied. This add a extra cost to the solution each time that\nmax_time_work is reached. It's like extra pay for the worker \neach time max_time_work is completed (journey).     people  int32, default=1   The number of people that you use in each route, for example if\nyou need two truck drivers. that's another contstraint. That\nwill multiply the time_costs and the extra_cost. Not the travel\ncost because it's assumed that both go in the same vehicle.",
            "title": "Hyper -  Parameters"
        },
        {
            "location": "/Simmulated_Annealing/#method",
            "text": ".fit(route_example, time_costs, fuel_costs) :  This method train the algorithm, we need to pass the following data: \n        1.  route_example : Is a list random route to initiate the algorithm.\n        2.  time_cost : it's a dict of dicts that represent a diagonal matrix with the times between all points\n        3.  fuel_cost : it's a dict of dicts that represent a diagonal matrix with the costs between all points",
            "title": "Method"
        },
        {
            "location": "/Simmulated_Annealing/#attribute",
            "text": "history_ :  it's a list with the best cost in each generation. The loss function.",
            "title": "Attribute"
        },
        {
            "location": "/Simmulated_Annealing/#examples-with-simulated-annealing",
            "text": "from sklearn_route.datasets import load_barcelona\nfrom sklearn_route.preprocessing import matrix_to_dict\nfrom sklearn_route.metaheuristics.simulated_annealing import SimulatedAnnealing\n\ndf_barcelona = load_barcelona()\n\n#Dataset - id of origin - id of destiny - column to transform (in this case the hour)\ntime_matrix = matrix_to_dict(df_barcelona, \"id_origin\", \"id_destinity\", \"hora\")\n\n#Dataset - id of origin - id of destiny - column to transform (in this case the cost)\ncost_matrix = matrix_to_dict(df_barcelona, \"id_origin\", \"id_destinity\", \"hora\")\n\n#Create a random route, it's will needed to initiate the algorithm\nroute_example = list(dict.fromkeys(cost_matrix_df).keys())\n\n#Instantiate the algorithm\nsa = SimulatedAnnealing(temp=10, neighbours=300, delta=0.82, tol=0.2,\n                        max_time_work=8, extra_cost=12.83, people=2)\n\n#random route - time_matrix - cost_matrix\nresult = sa.fit(route_example, time_matrix, cost_matrix)\n\n#Printing the best route\nprint(result)\n\n#Printing the loss function\nprint(sa.history_)",
            "title": "Examples with Simulated Annealing."
        },
        {
            "location": "/Simmulated_Annealing/#understunding-the-data-needed-by-the-algorithm",
            "text": "The  dicts of dicts  time_cost and fuel_cost, are dicts of dicts for performance reason. these dicst could be also represented as diagonal symmetric matrix, columns and index represent the cost of the point. Here an example:  easy_route = {\n            1:{\n                1:0,\n                2:x\n                3:y\n                },\n            2:{\n                1:x,\n                2:0,\n                3:z\n                },\n            3:{\n                1:y,\n                2:z,\n                3:0\n                }\n            }\n\n#We can see clearly the matrix with pandas\nimport pandas as pd\n\n#Visualice the diagonal symmetric matrix\nprint(pd.DataFrame(easy_route))  Output:   It's important to understund that the cost matrix  must be the computation of all the costs  that the user of the algorithm want to take in count  per person . In the majority of cases the cost will be composed by:   The cost of the fuel for one person to go from one point to another.  The cost of the hours of work needed from one point to another.   If we have multiple persons doing the same route, the  people  parameter will multiply the cost by the number of persons and will increase the cost with the  extra_cost  in case of the  max_time_work  paremeter will be surpased.  Also note that  extra_cost   could be used as a maximun capacity  of a truck for example in the case of material transports problems.  The other dict of dicts  time_matrix  parameter will be only used by the algorithm to compute the  max_time_work  and compute the final route of the algorithm with the times that the salesman/truck have finished their journey.",
            "title": "Understunding the data needed by the algorithm"
        },
        {
            "location": "/Simmulated_Annealing/#caveats-for-beginners",
            "text": "It's always common for beginners think \"If I increase the time, with high  temp , high  neighbours  and low  delta  \nthe results will be better\" This is not true, you have to find the optimal parameters for your especific problem.\nFor example if your data is compose by 4 differents places to visit, don't have sense to have 250 neighbours, because\nyou don't have a lot of differents combinations possible.  Other case it's if you have for example a too high temperature, and the places are very near one each other then\nprobably the algorithm will never converge because hight temperature will lead to not accept worst solutions when\nthe algorithm is converging.",
            "title": "Caveats for beginners"
        },
        {
            "location": "/Simmulated_Annealing/#ensemble-simulated-annealing",
            "text": "sklearn_route.simulated_annealing.EnsembleSimulatedAnnealing (n_simulateds=20,  temp=12.0, neighbours=250, delta=0.78, tol=1.29,\n            max_time_work=8.0, extra_cost=10.0, people=1, n_jobs=1)   Ensemble Simulated Annealing is a bagging of Simulated Annealing models, so you can refer to the documentation above to see how it works. Basically are a  X  number of Genetics estimator, this help to get a better result than the Simulated Annealing alogorithm, the computationall cost will depend of the number of workers (n_jobs) you use. In the case you use the same  n_jobs  as  n_simulateds  you will be as fast in time as Simulated Anealing  As said above, the algorith is pretty much the same, so here there are only the new hyper parameters.",
            "title": "Ensemble Simulated Annealing"
        },
        {
            "location": "/Simmulated_Annealing/#hyper-parameters_1",
            "text": "n_simulateds : int, default=10   The number of Simulated Annealing algorithms that will be thrown. The more algorithms better result could be achieve, but's will be computationally more expensive (this can be solved/mitigated with the n_jobs parameter)    n_jobs : int, default=1  The number of workers (threads) that will use the algorithm in parallel, by default is one, and must be at least one to the algorithm run. Use this parameter with caution, maybe can collapse the computer if you select a lot jobs.",
            "title": "Hyper - Parameters."
        },
        {
            "location": "/Simmulated_Annealing/#example-with-ensemble-simulated-annealing",
            "text": "from sklearn_route.datasets import load_valencia\nfrom sklearn_route.preprocessing import matrix_to_dict\nfrom sklearn_route.genetics import EnsembleGenetic\n\ndf_valencia = load_valencia()\n\n#Dataset - id of origin - id of destiny - column to transform (in this case the hour)\ntime_matrix = matrix_to_dict(df_valencia, \"id_origin\", \"id_destinity\", \"hora\")\n\n#Dataset - id of origin - id of destiny - column to transform (in this case the cost)\ncost_matrix = matrix_to_dict(df_valencia, \"id_origin\", \"id_destinity\", \"cost\")\n\n#Create a random route, it's will needed to initiate the algorithm\nroute_example = list(dict.fromkeys(cost_matrix_df).keys())\n\n#Instantiate the algorithm with 18 simulated annealing algorithm and 6 jobs\nesa = EnsembleSimulatedAnneling(n_simulateds=18, temp = 18, neighbours=300, max_time_work=6, extra_cost=12.83, n_jobs=6)\n\n#random route - time_matrix - cost_matrix\nresult = esa.fit(route_example, time_matrix, cost_matrix)\n\n#Printing the best route\nprint(result)",
            "title": "Example with Ensemble Simulated Annealing"
        },
        {
            "location": "/TabuSearch/",
            "text": "Genetics\n\n\nThe main advantage of this Tabu Search over others solutions are:\n* Easy to learn and to use, inspired in Scikit Learn library.\n* Very well documented.\n* Give very good results\n\n\nGlobal understunding of the Algorithm\n\n\nTabu Search is metaheuristic aproach wich mean is not deterministic this lead that every time you run the result could be different but will be very near of the \nglobal optimum.\n\n\nThe algorithm is called Tabu Search because it have a Tabu List, who makes that if a generated route have been prove recently, the algorithm will check another diferent not in the Tabu List. Also the algorithm have a probability to mutate the route, this create a reverse a subroute of the initial route, that make the algorithm visits other nears solutions.\n\n\nTabu Search\n\n\nHyper - Parameters\n\n\n\n\nsklearn_route.metaheuristics.tabu_search.TabuSearch\n(searchs=1250, p_m=0.6, tabu_length=45, tabu_var=10, max_time_work=8,\n                                                        people=1, extra_cost=0)\n\n\n\n\n\n\n\n\nsearchs\n: int, default=1250\n\n\n\n\nThe number of neighbourhood route searchs, more searchs will lead to a better result but with an associate time computation cost.\n\n\n\n\n\n\n\n\np_m\n: float, default=0.6\n\n\n\n\nit's mutate, random probabilies are choosen when it will take place. If the random numbers are 2 and 8 for example, the numbers located at that index will swap positions.\n\n\n\n\n\n\n\n\ntabu_length\n: int, default=45\n\n\n\n\nit's the lenght of tabu list. Tabu list is a list of routes that the alogirhtm is not going to visit because have been visited already\n\n\n\n\n\n\n\n\ntabu_var\n: int, default=10\n\n\n\n\nVariations in the lenght of tabu_length, can't be higher number than tabu_length\n\n\n\n\n\n\n\n\nmax_time_work\n: float32, default=8\n\n\n\n\nThe number of ours that a employ can work per day. For example\nif it's 8 hours, the algorithm will force that a route have to\nfinish after the 8 hours have been completed, making the\nemployeed come back home. it's a time constraint.\n\n\n\n\n\n\n\n\nextra_cost\n: float32, default=0\n\n\n\n\nIf it's 0 anything happend. If it's > 0 in combination with\nmax_time_work when the max_time_work is reached, extra_cost is\napplied. This add a extra cost to the solution each time that\nmax_time_work is reached. It's like extra pay for the worker \neach time max_time_work is completed (journey).\n\n\n\n\n\n\n\n\npeople\n int32, default=1\n\n\n\n\nThe number of people that you use in each route, for example if\nyou need two truck drivers. that's another contstraint. That\nwill multiply the time_costs and the extra_cost. Not the travel\ncost because it's assumed that both go in the same vehicle.\n\n\n\n\n\n\n\n\nMethod\n\n\n\n\n.fit(route_example, time_costs, fuel_costs)\n:\n\n\nThis method train the algorithm, we need to pass the following data:\n\n        1. \nroute_example\n: Is a list random route to initiate the algorithm.\n        2. \ntime_cost\n: it's a dict of dicts that represent a diagonal matrix with the times between all points\n        3. \nfuel_cost\n: it's a dict of dicts that represent a diagonal matrix with the costs between all points\n\n\n\n\n\n\n\n\nAttribute\n\n\n\n\nhistory_\n:\n\n\nit's a list with the best cost in each generation. The loss function.\n\n\n\n\n\n\n\n\nExample with Tabu Search Algorithm.\n\n\nfrom sklearn_route.datasets import load_barcelona\nfrom sklearn_route.preprocessing import matrix_to_dict\nfrom sklearn_route.metaheuristics.tabu_search import TabuSearch\n\ndf_barcelona = load_barcelona()\n\n#Dataset - id of origin - id of destiny - column to transform (in this case the hour)\ntime_matrix = matrix_to_dict(df_barcelona, \"id_origin\", \"id_destinity\", \"hora\")\n\n#Dataset - id of origin - id of destiny - column to transform (in this case the cost)\ncost_matrix = matrix_to_dict(df_barcelona, \"id_origin\", \"id_destinity\", \"hora\")\n\n#Create a random route, it's will needed to initiate the algorithm\nroute_example = list(dict.fromkeys(cost_matrix_df).keys())\n\n#Instantiate the algorithm\nts = TabuSearch(p_m = 0.3, pop=400, gen=2000, k=5, p_c early_stoping=100,\n            max_time_work=6, extra_cost=12.83)\n\n#random route - time_matrix - cost_matrix\nresult = ts.fit(route_example, time_matrix, cost_matrix)\n\n#Printing the best route\nprint(result)\n\n#Printing the loss function\nprint(ts.history_)\n\n\n\nUnderstunding the data needed by the algorithm\n\n\nThe \ndicts of dicts\n time_cost and fuel_cost, are dicts of dicts for performance reason. these dicst could be also represented as diagonal symmetric matrix, columns and index represent the cost of the point. Here an example:\n\n\neasy_route = {\n            1:{\n                1:0,\n                2:x\n                3:y\n                },\n            2:{\n                1:x,\n                2:0,\n                3:z\n                },\n            3:{\n                1:y,\n                2:z,\n                3:0\n                }\n            }\n\n#We can see clearly the matrix with pandas\nimport pandas as pd\n\n#Visualice the diagonal symmetric matrix\nprint(pd.DataFrame(easy_route))",
            "title": "TabuSearch"
        },
        {
            "location": "/TabuSearch/#genetics",
            "text": "The main advantage of this Tabu Search over others solutions are:\n* Easy to learn and to use, inspired in Scikit Learn library.\n* Very well documented.\n* Give very good results",
            "title": "Genetics"
        },
        {
            "location": "/TabuSearch/#global-understunding-of-the-algorithm",
            "text": "Tabu Search is metaheuristic aproach wich mean is not deterministic this lead that every time you run the result could be different but will be very near of the  global optimum.  The algorithm is called Tabu Search because it have a Tabu List, who makes that if a generated route have been prove recently, the algorithm will check another diferent not in the Tabu List. Also the algorithm have a probability to mutate the route, this create a reverse a subroute of the initial route, that make the algorithm visits other nears solutions.",
            "title": "Global understunding of the Algorithm"
        },
        {
            "location": "/TabuSearch/#tabu-search",
            "text": "",
            "title": "Tabu Search"
        },
        {
            "location": "/TabuSearch/#hyper-parameters",
            "text": "sklearn_route.metaheuristics.tabu_search.TabuSearch (searchs=1250, p_m=0.6, tabu_length=45, tabu_var=10, max_time_work=8,\n                                                        people=1, extra_cost=0)     searchs : int, default=1250   The number of neighbourhood route searchs, more searchs will lead to a better result but with an associate time computation cost.     p_m : float, default=0.6   it's mutate, random probabilies are choosen when it will take place. If the random numbers are 2 and 8 for example, the numbers located at that index will swap positions.     tabu_length : int, default=45   it's the lenght of tabu list. Tabu list is a list of routes that the alogirhtm is not going to visit because have been visited already     tabu_var : int, default=10   Variations in the lenght of tabu_length, can't be higher number than tabu_length     max_time_work : float32, default=8   The number of ours that a employ can work per day. For example\nif it's 8 hours, the algorithm will force that a route have to\nfinish after the 8 hours have been completed, making the\nemployeed come back home. it's a time constraint.     extra_cost : float32, default=0   If it's 0 anything happend. If it's > 0 in combination with\nmax_time_work when the max_time_work is reached, extra_cost is\napplied. This add a extra cost to the solution each time that\nmax_time_work is reached. It's like extra pay for the worker \neach time max_time_work is completed (journey).     people  int32, default=1   The number of people that you use in each route, for example if\nyou need two truck drivers. that's another contstraint. That\nwill multiply the time_costs and the extra_cost. Not the travel\ncost because it's assumed that both go in the same vehicle.",
            "title": "Hyper - Parameters"
        },
        {
            "location": "/TabuSearch/#method",
            "text": ".fit(route_example, time_costs, fuel_costs) :  This method train the algorithm, we need to pass the following data: \n        1.  route_example : Is a list random route to initiate the algorithm.\n        2.  time_cost : it's a dict of dicts that represent a diagonal matrix with the times between all points\n        3.  fuel_cost : it's a dict of dicts that represent a diagonal matrix with the costs between all points",
            "title": "Method"
        },
        {
            "location": "/TabuSearch/#attribute",
            "text": "history_ :  it's a list with the best cost in each generation. The loss function.",
            "title": "Attribute"
        },
        {
            "location": "/TabuSearch/#example-with-tabu-search-algorithm",
            "text": "from sklearn_route.datasets import load_barcelona\nfrom sklearn_route.preprocessing import matrix_to_dict\nfrom sklearn_route.metaheuristics.tabu_search import TabuSearch\n\ndf_barcelona = load_barcelona()\n\n#Dataset - id of origin - id of destiny - column to transform (in this case the hour)\ntime_matrix = matrix_to_dict(df_barcelona, \"id_origin\", \"id_destinity\", \"hora\")\n\n#Dataset - id of origin - id of destiny - column to transform (in this case the cost)\ncost_matrix = matrix_to_dict(df_barcelona, \"id_origin\", \"id_destinity\", \"hora\")\n\n#Create a random route, it's will needed to initiate the algorithm\nroute_example = list(dict.fromkeys(cost_matrix_df).keys())\n\n#Instantiate the algorithm\nts = TabuSearch(p_m = 0.3, pop=400, gen=2000, k=5, p_c early_stoping=100,\n            max_time_work=6, extra_cost=12.83)\n\n#random route - time_matrix - cost_matrix\nresult = ts.fit(route_example, time_matrix, cost_matrix)\n\n#Printing the best route\nprint(result)\n\n#Printing the loss function\nprint(ts.history_)",
            "title": "Example with Tabu Search Algorithm."
        },
        {
            "location": "/TabuSearch/#understunding-the-data-needed-by-the-algorithm",
            "text": "The  dicts of dicts  time_cost and fuel_cost, are dicts of dicts for performance reason. these dicst could be also represented as diagonal symmetric matrix, columns and index represent the cost of the point. Here an example:  easy_route = {\n            1:{\n                1:0,\n                2:x\n                3:y\n                },\n            2:{\n                1:x,\n                2:0,\n                3:z\n                },\n            3:{\n                1:y,\n                2:z,\n                3:0\n                }\n            }\n\n#We can see clearly the matrix with pandas\nimport pandas as pd\n\n#Visualice the diagonal symmetric matrix\nprint(pd.DataFrame(easy_route))",
            "title": "Understunding the data needed by the algorithm"
        },
        {
            "location": "/Tutorial/",
            "text": "Tutorial\n\n\nThis is a quick tutorial based on examples to learn to use quickly the differents algorithms of the library\n\n\nGenetic\n\n\nGenetic is a metaheuristic algorithm wich mean that it's not deterministic, so every time the algorithm output will be a different solution near of the unknown best solution.\n\n\nThis Algorithm is focused in solve the MTSPTW problem (\nM\nulti \nT\nravel \nS\nales \nP\nroblem with \nT\nime \nW\nindows). This problem is about a person that need to cover a number of points to visit but have a limit of working hours (time constraint) and extra cost. If it's the case, this algorithm is the perfect solution:\n\n\nLoad the modules\n\n\nFirst of all we load the modules that we will use:\n\n\nfrom sklearn_route.datasets import load_barcelona\nfrom sklearn_route.preprocessing import matrix_to_dict\nfrom sklearn_route.genetics import Genetic\n\n\n\nIf you want to know more about these modules you can visit:\n\n\n\n\nDatasets\n\n\nPreprocessin\n\n\nGenetics\n\n\n\n\nPre-processing data\n\n\nHere we put the data in the format required by the algorithm. In this case we use the \nmatrix_to_dict\n function to transform our \nid_points\n cost in fuel and in time to dicts of dicts.\nYou can see the docs of all functions, using the \nhelp()\n command. Also there are more detailed examples in the docs webpage.\n\n\ndf_barcelona = load_barcelona()\n\n#Dataset - id of origin - id of destiny - column to transform (in this case the hour)\ntime_matrix = matrix_to_dict(df_barcelona, \"id_origin\", \"id_destinity\", \"hora\")\n\n#Dataset - id of origin - id of destiny - column to transform (in this case the cost)\ncost_matrix = matrix_to_dict(df_barcelona, \"id_origin\", \"id_destinity\", \"hora\")\n\n#Create a random route, it's will needed to initiate the algorithm\nroute_example = list(dict.fromkeys(cost_matrix_df).keys())\n\n\n\nRunning the Algorithm\n\n\nHere we see how the algorithm operation is the same as Scikit Learn. You can see all the Hyperparamenters, what they do, and examples with the builtin \nhelp()\n function. Also there are more detailed examples in the docs webpage.\n\n\nIn this case we choose an \nalgorithm with Methaeuristic\n aproach to the problem. There are other algorithms in the package that can solve this problem or have another approach.\n\n\n#Instantiate the algorithm\nga = Genetic(p_m = 0.3, pop=400, gen=2000, k=5, p_c early_stoping=100, max_time_work=6, extra_cost=12.83)\n\n#random route - time_matrix - cost_matrix\nresult = ga.fit(route_example, time_matrix, cost_matrix)\n\n#Printing the best route\nprint(result)\n\n#Printing the loss function\nprint(ga.history_)\n\n\n\nIn the Genetic Algorithm we have a lot of Hyperparameters to config, for a full understanding of the Algorithm please refer to \nGenetics\n\n\nClustering\n\n\nGenetics\n\n\nSimmulated Annealing\n\n\nBrute Force\n\n\nUtilities",
            "title": "Tutorial"
        },
        {
            "location": "/Tutorial/#tutorial",
            "text": "This is a quick tutorial based on examples to learn to use quickly the differents algorithms of the library",
            "title": "Tutorial"
        },
        {
            "location": "/Tutorial/#genetic",
            "text": "Genetic is a metaheuristic algorithm wich mean that it's not deterministic, so every time the algorithm output will be a different solution near of the unknown best solution.  This Algorithm is focused in solve the MTSPTW problem ( M ulti  T ravel  S ales  P roblem with  T ime  W indows). This problem is about a person that need to cover a number of points to visit but have a limit of working hours (time constraint) and extra cost. If it's the case, this algorithm is the perfect solution:",
            "title": "Genetic"
        },
        {
            "location": "/Tutorial/#load-the-modules",
            "text": "First of all we load the modules that we will use:  from sklearn_route.datasets import load_barcelona\nfrom sklearn_route.preprocessing import matrix_to_dict\nfrom sklearn_route.genetics import Genetic  If you want to know more about these modules you can visit:   Datasets  Preprocessin  Genetics",
            "title": "Load the modules"
        },
        {
            "location": "/Tutorial/#pre-processing-data",
            "text": "Here we put the data in the format required by the algorithm. In this case we use the  matrix_to_dict  function to transform our  id_points  cost in fuel and in time to dicts of dicts.\nYou can see the docs of all functions, using the  help()  command. Also there are more detailed examples in the docs webpage.  df_barcelona = load_barcelona()\n\n#Dataset - id of origin - id of destiny - column to transform (in this case the hour)\ntime_matrix = matrix_to_dict(df_barcelona, \"id_origin\", \"id_destinity\", \"hora\")\n\n#Dataset - id of origin - id of destiny - column to transform (in this case the cost)\ncost_matrix = matrix_to_dict(df_barcelona, \"id_origin\", \"id_destinity\", \"hora\")\n\n#Create a random route, it's will needed to initiate the algorithm\nroute_example = list(dict.fromkeys(cost_matrix_df).keys())",
            "title": "Pre-processing data"
        },
        {
            "location": "/Tutorial/#running-the-algorithm",
            "text": "Here we see how the algorithm operation is the same as Scikit Learn. You can see all the Hyperparamenters, what they do, and examples with the builtin  help()  function. Also there are more detailed examples in the docs webpage.  In this case we choose an  algorithm with Methaeuristic  aproach to the problem. There are other algorithms in the package that can solve this problem or have another approach.  #Instantiate the algorithm\nga = Genetic(p_m = 0.3, pop=400, gen=2000, k=5, p_c early_stoping=100, max_time_work=6, extra_cost=12.83)\n\n#random route - time_matrix - cost_matrix\nresult = ga.fit(route_example, time_matrix, cost_matrix)\n\n#Printing the best route\nprint(result)\n\n#Printing the loss function\nprint(ga.history_)  In the Genetic Algorithm we have a lot of Hyperparameters to config, for a full understanding of the Algorithm please refer to  Genetics",
            "title": "Running the Algorithm"
        },
        {
            "location": "/Tutorial/#clustering",
            "text": "",
            "title": "Clustering"
        },
        {
            "location": "/Tutorial/#genetics",
            "text": "",
            "title": "Genetics"
        },
        {
            "location": "/Tutorial/#simmulated-annealing",
            "text": "",
            "title": "Simmulated Annealing"
        },
        {
            "location": "/Tutorial/#brute-force",
            "text": "",
            "title": "Brute Force"
        },
        {
            "location": "/Tutorial/#utilities",
            "text": "",
            "title": "Utilities"
        },
        {
            "location": "/Utilities/",
            "text": "",
            "title": "Utilities"
        },
        {
            "location": "/about/",
            "text": "",
            "title": "About"
        }
    ]
}